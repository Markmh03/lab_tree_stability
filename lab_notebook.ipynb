{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Trees and Model Stability\n",
    "\n",
    "Trees are notorious for being **unstable**: Small changes in the data can lead to noticeable or large changes in the tree. We're going to explore this phenomenon, and a common rebuttal.\n",
    "\n",
    "In the folder for this lab, there are three datasets that we used in class: Divorce, heart failure, and the AirBnB price dataset.\n",
    "\n",
    "1. Pick one of the datasets and appropriately clean it.\n",
    "2. Perform a train-test split for a specific seed (save the seed for reproducibility). Fit a classification/regression tree and a linear model on the training data and evaluate their performance on the test data. Set aside the predictions these models make.\n",
    "3. Repeat step 2 for three to five different seeds (save the seeds for reproducibility). How different are the trees that you get? Your linear model coefficients?. Set aside the predictions these models make.\n",
    "\n",
    "Typically, you would see the trees changing what appears to be a non-trivial amount, while the linear model coefficients don't vary nearly as much. Often, the changes appear substantial. \n",
    "\n",
    "But are they?\n",
    "\n",
    "4. Instead of focusing on the tree or model coefficients, do three things:\n",
    "    1. Make scatterplots of the predicted values on the test set from question 2 against the predicted values for the alternative models from part 3, separately for your trees and linear models. Do they appear reasonably similar?\n",
    "    2. Compute the correlation between your model in part 2 and your alternative models in part 3, separately for your trees and linear models. Are they highly correlated or not?\n",
    "    3. Run a simple linear regression of the predicted values on the test set from the alternative models on the predicted values from question 2, separately for your trees and linear models. Is the intercept close to zero? Is the slope close to 1? Is the $R^2$ close to 1?\n",
    "\n",
    "5. Do linear models appear to have similar coefficients and predictions across train/test splits? Do trees?\n",
    "6. True or false, and explain: \"Even if the models end up having a substantially different appearance, the predictions they generate are often very similar.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after cleaning: (29831, 12)\n",
      "Numeric columns: 8 | Categorical columns: 5\n",
      "Saved cleaned dataset to: /workspaces/lab_tree_stability/data/airbnb_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# PART 1 — Load & clean the AirBnB dataset (self-contained, idempotent)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "#paths\n",
    "RAW_PATHS = [Path(\"data/airbnb_hw.csv\"), Path(\"/mnt/data/airbnb_hw.csv\")]\n",
    "RAW_PATH = next((p for p in RAW_PATHS if p.exists()), None)\n",
    "assert RAW_PATH is not None, \"Could not find airbnb_hw.csv in data/ or /mnt/data/.\"\n",
    "\n",
    "CLEAN_PATH = Path(\"data/airbnb_clean.csv\")\n",
    "\n",
    "#load\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "\n",
    "#normalize column names\n",
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.strip()\n",
    "      .str.lower()\n",
    "      .str.replace(r\"[^0-9a-zA-Z]+\", \"_\", regex=True)\n",
    "      .str.strip(\"_\")\n",
    ")\n",
    "\n",
    "#helpers\n",
    "def to_num_currency(s):\n",
    "    return pd.to_numeric(\n",
    "        s.astype(str).str.replace(r\"[\\$,]\", \"\", regex=True).str.strip(),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "def to_num_percent(s):\n",
    "    out = pd.to_numeric(s.astype(str).str.replace(\"%\", \"\", regex=False), errors=\"coerce\")\n",
    "    return out / 100.0\n",
    "\n",
    "#parse common fields if present\n",
    "for c in [c for c in df.columns if c in [\"price\",\"weekly_price\",\"monthly_price\",\"cleaning_fee\",\"security_deposit\"]]:\n",
    "    df[c] = to_num_currency(df[c])\n",
    "\n",
    "for c in [c for c in df.columns if c in [\"host_response_rate\",\"host_acceptance_rate\",\"occupancy_rate\"]]:\n",
    "    df[c] = to_num_percent(df[c])\n",
    "\n",
    "for c in [c for c in df.columns if c.startswith(\"review_scores\")]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "#deduplicate had AI help me catch this almost mistake\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# drop columns with too much missingness=\n",
    "missing_frac = df.isna().mean()\n",
    "to_drop = missing_frac[missing_frac > 0.40].index.tolist()\n",
    "if to_drop:\n",
    "    df = df.drop(columns=to_drop)\n",
    "\n",
    "#target presence & obvious invalid rows\n",
    "if \"price\" in df.columns:\n",
    "    df = df[pd.notna(df[\"price\"]) & (df[\"price\"] > 0)]\n",
    "    # robust outlier trimming on price\n",
    "    q1, q3 = df[\"price\"].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    upper = q3 + 3.0 * iqr\n",
    "    lower = max(q1 - 3.0 * iqr, 0)\n",
    "    df = df[(df[\"price\"] >= lower) & (df[\"price\"] <= upper)]\n",
    "\n",
    "#impute remaining missing values\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "for c in cat_cols:\n",
    "    if df[c].isna().any():\n",
    "        mode = df[c].mode(dropna=True)\n",
    "        df[c] = df[c].fillna(mode.iloc[0] if not mode.empty else \"unknown\")\n",
    "\n",
    "#drop obvious ID-like columns if present\n",
    "for id_like in [\"id\",\"listing_id\",\"host_id\",\"scrape_id\"]:\n",
    "    if id_like in df.columns:\n",
    "        df = df.drop(columns=id_like)\n",
    "\n",
    "# -------- save & report --------\n",
    "print(\"Shape after cleaning:\", df.shape)\n",
    "print(\"Numeric columns:\", len(num_cols), \"| Categorical columns:\", len(cat_cols))\n",
    "df.to_csv(CLEAN_PATH, index=False)\n",
    "print(f\"Saved cleaned dataset to: {CLEAN_PATH.resolve()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Linear | RMSE 61.08  MAE 42.90  R²  0.494\n",
      "    Tree | RMSE 69.22  MAE 45.95  R²  0.350\n",
      "Saved: data/preds_part2_airbnb.csv\n"
     ]
    }
   ],
   "source": [
    "# PART 2 — split + tree vs linear (robust to sklearn version, memory safe)\n",
    "#had issue of my kernal crashing before so I had to truncate this version by not creating a giant matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "#settings\n",
    "SEED = 2025\n",
    "CLEAN = Path(\"data/airbnb_clean.csv\")   # produced in Part 1\n",
    "assert CLEAN.exists(), \"Run Part 1 first to create data/airbnb_clean.csv\"\n",
    "\n",
    "#load & basic X/y\n",
    "df = pd.read_csv(CLEAN)\n",
    "TARGET = \"price\"\n",
    "assert TARGET in df.columns\n",
    "X = df.drop(columns=[TARGET], errors=\"ignore\").copy()\n",
    "y = df[TARGET].astype(float)\n",
    "\n",
    "# Drop obvious non-features if present (ids/long text or dates that slipped through)\n",
    "for c in [\"id\",\"listing_id\",\"host_id\",\"scrape_id\",\"host_since\",\"name\",\"description\",\"neighborhood_overview\"]:\n",
    "    if c in X.columns:\n",
    "        X.drop(columns=c, inplace=True)\n",
    "\n",
    "#limit categorical OHE to low-cardinality\n",
    "raw_cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "card = X[raw_cat_cols].nunique(dropna=True) if raw_cat_cols else pd.Series(dtype=int)\n",
    "LOW_CARD_MAX = 50\n",
    "low_card_cats = [c for c in raw_cat_cols if card[c] <= LOW_CARD_MAX]\n",
    "dropped_cats  = sorted(set(raw_cat_cols) - set(low_card_cats))\n",
    "if dropped_cats:\n",
    "    print(\"Dropping high-cardinality/text columns:\", dropped_cats)\n",
    "    X.drop(columns=dropped_cats, inplace=True)\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = low_card_cats  # may be []\n",
    "\n",
    "#split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=SEED\n",
    ")\n",
    "\n",
    "#preprocessors needed AI help on this section\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "])\n",
    "\n",
    "# sklearn version compatibility: prefer sparse_output, fall back to sparse\n",
    "try:\n",
    "    _ = OneHotEncoder(sparse_output=True)\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", ohe),\n",
    "])\n",
    "\n",
    "preprocess_sparse = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0  # keep sparse if any block is sparse\n",
    ")\n",
    "\n",
    "#models, inspired from class notes\n",
    "lin = Pipeline([\n",
    "    (\"pre\", preprocess_sparse),              # LinearRegression can take CSR sparse\n",
    "    (\"model\", LinearRegression()),\n",
    "])\n",
    "\n",
    "to_dense = FunctionTransformer(\n",
    "    lambda Z: Z.toarray() if hasattr(Z, \"toarray\") else Z, accept_sparse=True\n",
    ")\n",
    "\n",
    "tree = Pipeline([\n",
    "    (\"pre\", preprocess_sparse),\n",
    "    (\"to_dense\", to_dense),                  # densify only for tree\n",
    "    (\"model\", DecisionTreeRegressor(random_state=SEED)),\n",
    "])\n",
    "\n",
    "#fit\n",
    "lin.fit(X_train, y_train)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#evaluate\n",
    "def report(name, y_true, y_pred):\n",
    "    try:\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:  # older sklearn without 'squared' kwarg\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    print(f\"{name:>8} | RMSE {rmse:,.2f}  MAE {mae:,.2f}  R² {r2: .3f}\")\n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "yhat_lin  = lin.predict(X_test)\n",
    "yhat_tree = tree.predict(X_test)\n",
    "\n",
    "m_lin  = report(\"Linear\", y_test, yhat_lin)\n",
    "m_tree = report(\"Tree\",   y_test, yhat_tree)\n",
    "\n",
    "# -------------------- save predictions for next parts --------------------\n",
    "preds = pd.DataFrame(\n",
    "    {\"y_true\": y_test.values, \"y_pred_linear\": yhat_lin, \"y_pred_tree\": yhat_tree},\n",
    "    index=y_test.index\n",
    ")\n",
    "preds.to_csv(\"data/preds_part2_airbnb.csv\", index=True)\n",
    "print(\"Saved: data/preds_part2_airbnb.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
